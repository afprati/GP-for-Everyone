{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9443454e-fee1-45bf-a9c8-a88756380499",
   "metadata": {},
   "source": [
    "This notebook describes the steps for the White nationalist rhetoric example (first application). Data is from Siegel et al 2021. Prepared by Annamaria Prati and Yehu Chen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab383ae-4a41-45b3-a49f-fb64e694e287",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Let's set ourselves up. First, load the required libraries. Note that `torch` should be version 2.6.0 and `gpytorch` should be version 1.8.1. \n",
    "\n",
    "Next, set a seed so that the results are consistent each time. We will also set the default data type to be `float64`, which retains two decimal points (helps the matrix operations) and helps prevent data type mismatches. \n",
    "\n",
    "Finally, load the dataset. We have provided the dataset in a subfolder for this example on the Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25922ef4-a390-4f20-a502-d28acd00f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "# load required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "print(torch.__version__)\n",
    "print(gpytorch.__version__)\n",
    "\n",
    "# set a random seed so results are consistent each time you run the code\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# set default data type to float64 for accuracy\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# load the dataset with the daily proportion of white nationalist tweets\n",
    "# this file should be in a folder called 'data' with the name 'white_nationalist_random.csv'\n",
    "data = pd.read_csv(\"./data/white_nationalist_random.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6691cb-a05e-4a20-bbe4-820ad7b66c1a",
   "metadata": {},
   "source": [
    "# Interrupted Time Series Framework\n",
    "\n",
    "Let's orient ourselves mathematically. \n",
    "\n",
    "Let $y_t$ be the proportion of white nationalist tweets at time $t$, $T$ indicate time, and  $D_t$ be a dummy variable indicating whether it is before ($D_t=0) $ or after ($D_t=1$) Election Day. The main model of interest is:\n",
    "\n",
    "$$y_t=\\beta_0 + \\beta_1(T) + \\beta_2(D_t) + \\beta_3 (D_tT).$$\n",
    "\n",
    "In the interrupted time series analysis framework, this model allows researchers to test the claim of a persistent change in speech after Election Day -- both in terms of level ($\\beta_2$) and slope ($\\beta_3$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc25b4-201c-4602-b2fa-d46eab808a3f",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Now we need to prepare our data for the model. First, we will change the date so that it is an integer representing the number of time periods that have passed since the start of the dataset. We do this since Gaussian processes work best when inputs are numeric. \n",
    "\n",
    "Next, we will rescale the values for the outcome variable $y$ so that they have better numerical performance. \n",
    "\n",
    "Third, we will create our input features: time, when \"treatment\" starts (i.e. the election happened), and the interaction between the two. \n",
    "\n",
    "Finally, we will find the time index for election day. This will faciliate our examination of what happens before or after treatment later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b809b54b-13ca-4e3a-bccf-8ba027f19dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column to number of days since the start of the dataset\n",
    "xs = data.date\n",
    "xs = xs.apply(lambda x: (datetime.strptime(x,\"%Y-%m-%d\")-datetime.strptime(xs[0],\"%Y-%m-%d\")).days)\n",
    "\n",
    "# rescale the values for better numerical performance\n",
    "y_scale = 1e6\n",
    "y = torch.tensor(data.norm_hate.values).double() * y_scale\n",
    "\n",
    "# create the input features: time, election indicator, and their interaction\n",
    "x = torch.tensor(np.array([xs.values, data.election.values, xs.values*data.election.values]).T).double()\n",
    "\n",
    "# find the index of election day\n",
    "election_day_index = np.where(x[:,1])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dea623-377b-40c2-a29e-2f89e2ef88f1",
   "metadata": {},
   "source": [
    "# Why Gaussian Process Regression?\n",
    "\n",
    "Instead of focusing on regression coefficients ($\\beta_0, \\beta_1, \\beta_2, \\beta_3$), Gaussian Process Regression allows us to estimate the function $f(t)$ modeling tweet proportions directly by constructing an appropriate model and prior as well as calculating the full posterior.\n",
    "\n",
    "This shifts the emphasis from parametric inference toward the substantive question of interest: Did the frequency of white nationalist tweets increase after Election Day?\n",
    "\n",
    "Formally, we estimate the post-election effect as the average difference between the GPR prediction for observed post-election outcomes and the counterfactual prediction of what would have been observed had there been no treatment effect, controlling for pre-election trends.\n",
    "\n",
    "That is,\n",
    "\n",
    "$$\n",
    "\\text{Effect} = \\frac{\\sum_{t>E}\\big(f_{t}|D_{t}=1, \\mathbf{X}_{-t} -  f^\\ast_i | D_{t}=0, \\mathbf{X}_{-t} \\big)}{\\sum_t I(t>E)} \n",
    "$$\n",
    "where $E$ indicates Election Day and $I(\\cdot)$ is the usual indicator function. This is the averaged difference between our GPR for the predicted value of $f_{t}$ and the predicted value we would have observed for that day factoring out any linear ``bump'' $(f_t^\\ast)$ for the post-election period. This approach directly recovers the average treatment effect of Election Day on speech, while avoiding strict linearity assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41f866-9a12-4759-a46a-b6131d641258",
   "metadata": {},
   "source": [
    "# Model Specification\n",
    "\n",
    "The general math \"behind the scenes\" is as follows. As explained in detail in the paper, a Gaussian Process is defining a distribution over functions, meaning we assume \n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))\n",
    "$$\n",
    "\n",
    "In this particular model, the mean function is linear to capture the overall trend in the data:  \n",
    "  $$\n",
    "  m(x) = \\beta_0 + \\beta_1 x\n",
    "  $$\n",
    "Based on the way we set up our input features, we know that our specific mean function is:\n",
    "    $$\n",
    "    \\mu(T, D_t) = \\beta_0 + \\beta_1 T + \\beta_2 D_t + \\beta_3 D_t T\n",
    "    $$\n",
    "\n",
    "- $T$ is time (in days)\n",
    "- $D_t$ is an indicator for whether it's election day or after (a binary variable)\n",
    "- this form allows for:\n",
    "    - a linear time trend ($\\beta_1 T$)\n",
    "    - a level shift on election day ($\\beta_2 D_t$)\n",
    "    - and an interaction ($\\beta_3 D_t T$), allowing the trend to change after the election\n",
    "\n",
    "\n",
    "And the covariance function (or kernel) is the RBF (also called the Squared Exponential, or SE) kernel:\n",
    "  $$\n",
    "  k(x, x') = \\sigma^2 \\exp\\left( -\\frac{(x - x')^2}{2\\ell^2} \\right)\n",
    "  $$\n",
    "\n",
    "Recall that this controls how much two observations are expected to move together based on how close they are in time (as specified by the `active_dims=[0]` portion of our code).  \n",
    "\n",
    "  $\\ell$ is the lengthscale — it determines how fast correlation decays with distance.  \n",
    "  $\\sigma^2$ is the outputscale — it controls the overall variation in the function.\n",
    "\n",
    "Together, this GPR prior models the function $f(x)$ as having a linear trend and smooth deviations from that trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19c461-b033-4a9e-b6c8-b50ada9d811a",
   "metadata": {},
   "source": [
    "# Constructing and Initializing the `GPModel` Class in `gpytorch`\n",
    "\n",
    "To operationalize this in `gpytorch`, we begin by defining our customized Gaussian process model as a class. It inherits properties from the off-the-shelf `ExactGP` model (from `gpytorch`), which is appropriate when we have a full (or randomly sampled) dataset and can compute exact inference (as opposed to approximation methods). \n",
    "\n",
    "After initilization, we define the mean function and kernel function. We set the mean function to be linear, which assumes that this is the baseline trend. The GP will be modeling deviations from this baseline trend. The `input_size` here is dynamically defined but we know it is equal to 3: time, the election indicator, and the interaction. For the kernel/covariance function, we use the RBF kernel, which assumes smooth changes. This is modeling how outputs relate to each other over time. By specifying `active_dims[0]` we have instructed the covariance function to only use the first input column, which is our time since election data. Note that this is NOT based on the election status at all.\n",
    "\n",
    "In the second part of the class definition, we instruct how the model should make predictions for any input. The instructions here say to compute the mean at each input using the linear trend we defined earlier; and to compute the covariance using the kernel and time column as we defined earlier. The model then returns the predictions as a multivariate normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d96b121-1315-44b5-a7ee-b1d1cf5c8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a gaussian process model with a linear mean and RBF (smooth) kernel\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    # initialize our method: takes training data and the likelihood model\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        # linear mean function: a straight line trend\n",
    "        self.mean_module = LinearMean(input_size=train_x.shape[1], bias=True)\n",
    "        # covariance function: RBF kernel for capturing smooth patterns over time\n",
    "        self.covar_module = ScaleKernel(RBFKernel(active_dims=[0]))\n",
    "\n",
    "    # define what the model does when making predictions\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dbf78-9ddf-4199-bed9-9568c0d5f76c",
   "metadata": {},
   "source": [
    "Now that we have defined our custom Gaussian process model class (`GPModel`), we assign the likelihood to be Gaussian (which assumes normally distributed noise), and then we create an instance of the model (called `model`) using our inputs $x$, outputs $y$, and the likelihood.\n",
    "\n",
    "This corresponds to the the model:\n",
    "\n",
    "$$\n",
    "y_t \\sim \\mathcal{N}(f(x_t), \\sigma_{\\text{noise}}^2)\n",
    "$$\n",
    "\n",
    "where $f(x_t)$ is drawn from a GP prior defined by the mean and kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "727e7ca3-9e03-4d7e-9dd2-be34598d5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(x, y, likelihood).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9952e2d-a061-4224-8a5e-5dd362bf587b",
   "metadata": {},
   "source": [
    "Now we set the initial values for the model's hyperparameters in a dictionary. At a high level, these are initial \"guesses\" for a hyperparameter. The model will update these during training based on the data (except for the lengthscale, more on that soon). \n",
    "\n",
    "We set three `mean_module.weights` since we have three input features: time, election indicator, and the interaction. By setting the initial values to zero, the model starts by assuming no trend or treatment effect -- this will be learned from the data.\n",
    "\n",
    "The `covar_module.outputscale` is the output scale, controlling overall variability of the Gaussian process. 1 is a \"medium-sized\" guess.\n",
    "\n",
    "The `covar_module.base_kernel.lengthscale` is the lengthscale, controlling the smoothness of the Gaussian process. It is set to `torch.tensor([90.])` to reflect that we believe opinions within a 90-day window should be similar. This is because we believe that hate speech trends in general should be slow to change. This value is later frozen for the test of whether hate speech has a \"jump\" after the election.\n",
    "\n",
    "Finally, `likelihood.noise` is a noise parameter that is the amount of random noise in the observations. \n",
    "\n",
    "With all the initial values for the hyperparameters set, we call the model’s `.initialize()` method and pass the values using `**` to unpack the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8083824-95ef-4537-8b86-10564cb7282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial values for model parameters\n",
    "hypers = {\n",
    "    'mean_module.weights': torch.tensor([0,0,0]),\n",
    "    'covar_module.outputscale': 1,\n",
    "    'covar_module.base_kernel.lengthscale': torch.tensor([90.]),\n",
    "    'likelihood.noise': 1,\n",
    "}    \n",
    "model = model.initialize(**hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e042a0-25e9-402d-9dab-85b2c6374601",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "We switch the model and likelihood into \"training\" mode. In `gpytorch`, this enables parameter updates during optimization. \n",
    "\n",
    "At this point we will also freeze the lengthscale value so that it is not updated, but we will optimize the other model parameters using the Adam optimization algorithm (a popular choice) with a learning rate of 0.05. Higher values for the learning rate mean larger steps. This is faster, but potentially less stable. Lower values for the learning rate mean smaller steps, which is slower. Note that we have wrapped the model's parameters as a `set` to de-duplicate any parameters -- this is technically not necessary but a safer option given our customization might become more complicated.\n",
    "\n",
    "Finally we define the loss function to marginal log likelihood for exact Gaussian process models. We will need this later for assessing the quality of our predictions.\n",
    "\n",
    "Mathematically, we want to maximize the marginal log-likelihood of the data under the GP prior:\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\theta}, \\boldsymbol{\\beta}) = -\\frac{1}{2} (\\mathbf{y} - \\mu(\\mathbf{X}))^\\top K_{\\theta}^{-1} (\\mathbf{y} - \\mu(\\mathbf{X})) - \\frac{1}{2} \\log |K_{\\theta}| - \\frac{n}{2} \\log 2\\pi\n",
    "$$\n",
    "\n",
    "- $K_{\\theta}$ is the covariance matrix defined by the kernel and its hyperparameters $\\boldsymbol{\\theta}$.\n",
    "- $\\mu(\\mathbf{X})$ is the mean vector defined by the linear mean function.\n",
    "- $\\boldsymbol{\\beta}$ are the parameters of the mean function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c0b256f-b814-4adc-89bc-cb6155e4be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# freeze lengthscale so it doesn't get updated\n",
    "model.covar_module.base_kernel.raw_lengthscale.requires_grad = False\n",
    "\n",
    "# set up optimizer to update the model parameters\n",
    "optimizer = torch.optim.Adam(set(model.parameters()), lr=0.05)\n",
    "\n",
    "# define the loss function for gaussian processes\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea557c8e-32f0-4a78-a3ae-05a6eed77dbd",
   "metadata": {},
   "source": [
    "Now we train the model! We have set it for 500 iterations or learning steps (you'll probably want fewer while building out the model, but probably more for finalized results) and we will save our losses in a list in case we need it later. Note that this might take awhile, but we have set it to print an update for you ever 50 iterations.\n",
    "\n",
    "`output` is calling the `forward` method in `GPModel` to compute the predictions for input $x$ (time, election indicator, and the interaction).\n",
    "\n",
    "`loss` is calculating the negative marginal log likelihood between the model's predictions and our true values of $y$. A smaller loss is better. This is equivalent to maximizing the log likelihood (because `pytorch` default is to minimize). `loss.backward()` computes the gradients of the loss with respect to the model parameters, instructing the optimizer which direction to move the parameters to reduce loss. This is appended to a list.\n",
    "\n",
    "Finally, `optimizer.step()` updates the model parameters using the gradients from `loss.backward()` and the learning rate (`lr=0.05`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50d4bc7d-3878-4656-bfdd-df72d667f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0/500 - Loss: 2.873 \n",
      "Iter 50/500 - Loss: 2.233 \n",
      "Iter 100/500 - Loss: 2.146 \n",
      "Iter 150/500 - Loss: 2.123 \n",
      "Iter 200/500 - Loss: 2.114 \n",
      "Iter 250/500 - Loss: 2.109 \n",
      "Iter 300/500 - Loss: 2.107 \n",
      "Iter 350/500 - Loss: 2.105 \n",
      "Iter 400/500 - Loss: 2.104 \n",
      "Iter 450/500 - Loss: 2.104 \n"
     ]
    }
   ],
   "source": [
    "# run the training loop\n",
    "training_iter = 500\n",
    "losses = []\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad() # resets gradients before each training step\n",
    "    output = model(x)\n",
    "    loss = -mll(output, y)\n",
    "    loss.backward()\n",
    "    if i % 50 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f '  % (i , training_iter, loss.item())) # loss from pytorch tensor\n",
    "    losses.append(loss.item())\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f72382-7497-46f2-8b5e-91de42434166",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "\n",
    "Training is now done. We freeze our learned hyperparameters by switching the model and likelihood into \"evaluation\" mode for predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aea4409-f05b-4297-b962-64f7b3c00e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianLikelihood(\n",
       "  (noise_covar): HomoskedasticNoise(\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch model to evaluation mode for predictions\n",
    "model.eval()\n",
    "likelihood.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244cb40-1535-456a-8338-a741c21c2c33",
   "metadata": {},
   "source": [
    "To obtain the posterior predictions for our observed data, we specify that we do not need to track gradients (`torch.no_grad()`, saves memory and computation time) and enable a setting that allows for faster computation of the prediction variance (`gpytorch.settings.fast_pred_var()`, at the cost of some precision). \n",
    "\n",
    "`out` is calling the `forward` method in `GPModel`, essentially calculating the posterior distribution for $x$ (time, the election indicator, and the interaction) and returning a multivariate normal distribution. \n",
    "\n",
    "`loss` is then calculating the negative marginal log likelihood of the predictions. Recall that `output` is defined when we trained the model. The `np.log(y_scale)` is to account for our previous scaling of the $y$ values (for stability purposes).  \n",
    "\n",
    "`mu_f` is the mean of the posterior distribution. \n",
    "\n",
    "`lower` and `upper` are the lower and upper bounds of the confidence interval for the posterior predictions. \n",
    "\n",
    "Note that this step is necessary for us to calculate our performance statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51ddcbb8-81c4-4cf6-a601-0b9f52122b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miame\\anaconda3\\Lib\\site-packages\\gpytorch\\models\\exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get posterior predictions for observed data\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = model(x) # posterior distribution for input x\n",
    "    loss = -mll(output, y)-np.log(y_scale)\n",
    "    mu_f = out.mean.numpy()\n",
    "    lower, upper = out.confidence_region()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca537fd-b9d7-464a-8207-77bc858cc90c",
   "metadata": {},
   "source": [
    "As discussed in the main text, as social scientists we are most likely not interested in the predictions but rather the difference between two scenarios: what if the election happened (observed) versus what if the election had not happened (counterfactual). We use our new prediction tool to model these two scenarios.  We do this by creating two copies of the inputs and changing the election indicator column such that one input is where the election occurred (i.e. the election indicator is 1) and one input is where the election did not occur (i.e. the election indicator is 0). \n",
    "\n",
    "Like before, we then get the posterior predictions for both sets of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "989f45a3-bcb5-4b21-82ed-c6ec54c55613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create counterfactual inputs: what if election had happened / not happened\n",
    "# we create two sets of inputs with election set to 1 and 0\n",
    "x_election_1 = x.clone().detach().requires_grad_(False)\n",
    "x_election_1[:,1] = 1\n",
    "x_election_0 = x.clone().detach().requires_grad_(False)\n",
    "x_election_0[:,1] = 0\n",
    "\n",
    "# get model predictions under both counterfactual scenarios\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out1 = model(x_election_1)\n",
    "    out0 = model(x_election_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570ec95-f21f-4ba0-a276-8c119293b0e5",
   "metadata": {},
   "source": [
    "We can now estimate the instantaneous effect of the election on election day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aad98830-b25f-42d9-8f01-9c5b22c86311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instaneous shift on election day: 7.35E-07 +- 2.79E-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estimate the instantaneous effect of election day\n",
    "effect = out1.mean.numpy()[election_day_index+1]-out0.mean.numpy()[election_day_index-1]\n",
    "effect_std = np.sqrt((out1.variance.detach().numpy()[election_day_index+1] +\n",
    "                      out0.variance.detach().numpy()[election_day_index-1]))\n",
    "print(\"instaneous shift on election day: {:.2E} +- {:.2E}\\n\".format(effect/y_scale, effect_std/y_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ec220-5b0d-41fe-a4cb-bb54ede555f1",
   "metadata": {},
   "source": [
    "We can now also estimate the ATT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40de25ac-7938-46d6-8e29-a6fd7878e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATT: 8.78E-07 +- 3.04E-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estimate the average treatment effect on the treated (post-election period only)\n",
    "x_no_effect = x.clone().detach().requires_grad_(False)\n",
    "x_no_effect[x[:,1]==1,1] = 0\n",
    "x_no_effect[x[:,1]==1,2] = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    out0 = model(x_no_effect)\n",
    "    mu_f0 = out0.mean.numpy()\n",
    "    lower0, upper0 = out0.confidence_region()\n",
    "\n",
    "mask1 = (x[:,0] >= election_day_index ) & (x[:,1]==1)\n",
    "effect = out1.mean.numpy()[mask1].mean()-out0.mean.numpy()[mask1].mean()\n",
    "effect_std = np.sqrt((out1.variance.detach().numpy()[mask1].mean() +\n",
    "                      out0.variance.detach().numpy()[mask1].mean()))\n",
    "\n",
    "print(\"ATT: {:.2E} +- {:.2E}\\n\".format(effect/y_scale, effect_std/y_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43e8f5-a3d5-41c8-8753-6d505f08b875",
   "metadata": {},
   "source": [
    "Or we can calculate the difference between the observed values and the calculated counterfactual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1578c26c-f081-4507-b3b6-41e939b1c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.73725149 0.73854109 0.7398307  0.7411203  0.74240991\n",
      " 0.74369951 0.74498911 0.74627872 0.74756832 0.74885793 0.75014753\n",
      " 0.75143713 0.75272674 0.75401634 0.75530594 0.75659555 0.75788515\n",
      " 0.75917476 0.76046436 0.76175396 0.76304357 0.76433317 0.76562278\n",
      " 0.76691238 0.76820198 0.76949159 0.77078119 0.77207079 0.7733604\n",
      " 0.77465    0.77593961 0.77722921 0.77851881 0.77980842 0.78109802\n",
      " 0.78238763 0.78367723 0.78496683 0.78625644 0.78754604 0.78883565\n",
      " 0.79012525 0.79141485 0.79270446 0.79399406 0.79528366 0.79657327\n",
      " 0.79786287 0.79915248 0.80044208 0.80173168 0.80302129 0.80431089\n",
      " 0.8056005  0.8068901  0.8081797  0.80946931 0.81075891 0.81204851\n",
      " 0.81333812 0.81462772 0.81591733 0.81720693 0.81849653 0.81978614\n",
      " 0.82107574 0.82236535 0.82365495 0.82494455 0.82623416 0.82752376\n",
      " 0.82881336 0.83010297 0.83139257 0.83268218 0.83397178 0.83526138\n",
      " 0.83655099 0.83784059 0.8391302  0.8404198  0.8417094  0.84299901\n",
      " 0.84428861 0.84557821 0.84686782 0.84815742 0.84944703 0.85073663\n",
      " 0.85202623 0.85331584 0.85460544 0.85589505 0.85718465 0.85847425\n",
      " 0.85976386 0.86105346 0.86234306 0.86363267 0.86492227 0.86621188\n",
      " 0.86750148 0.86879108 0.87008069 0.87137029 0.8726599  0.8739495\n",
      " 0.8752391  0.87652871 0.87781831 0.87910792 0.88039752 0.88168712\n",
      " 0.88297673 0.88426633 0.88555593 0.88684554 0.88813514 0.88942475\n",
      " 0.89071435 0.89200395 0.89329356 0.89458316 0.89587277 0.89716237\n",
      " 0.89845197 0.89974158 0.90103118 0.90232078 0.90361039 0.90489999\n",
      " 0.9061896  0.9074792  0.9087688  0.91005841 0.91134801 0.91263762\n",
      " 0.91392722 0.91521682 0.91650643 0.91779603 0.91908563 0.92037524\n",
      " 0.92166484 0.92295445 0.92424405 0.92553365 0.92682326 0.92811286\n",
      " 0.92940247 0.93069207 0.93198167 0.93327128 0.93456088 0.93585048\n",
      " 0.93714009 0.93842969 0.9397193  0.9410089  0.9422985  0.94358811\n",
      " 0.94487771 0.94616732 0.94745692 0.94874652 0.95003613 0.95132573\n",
      " 0.95261534 0.95390494 0.95519454 0.95648415 0.95777375 0.95906335\n",
      " 0.96035296 0.96164256 0.96293217 0.96422177 0.96551137 0.96680098\n",
      " 0.96809058 0.96938019 0.97066979 0.97195939 0.973249   0.9745386\n",
      " 0.9758282  0.97711781 0.97840741 0.97969702 0.98098662 0.98227622\n",
      " 0.98356583 0.98485543 0.98614504 0.98743464 0.98872424 0.99001385\n",
      " 0.99130345 0.99259305 0.99388266 0.99517226 0.99646187 0.99775147\n",
      " 0.99904107 1.00033068 1.00162028 1.00290989 1.00419949 1.00548909\n",
      " 1.0067787  1.0080683  1.0093579  1.01064751 1.01193711 1.01322672\n",
      " 1.01451632 1.01580592 1.01709553 1.01838513]\n"
     ]
    }
   ],
   "source": [
    "# calculate the difference between the factual and counterfactual predictions\n",
    "diff = out.mean.numpy() - out0.mean.numpy()\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bcd41-2cd4-4301-a521-926ba17afacd",
   "metadata": {},
   "source": [
    "We can now examine model fit metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb0ad65d-dbb6-439b-b6c6-6cabc1c030fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001959731542149116\n",
      "log lik: 8549.6996 \n",
      "\n",
      "BIC: -17053.248 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate model fit metrics\n",
    "BIC = (3+2+2)*torch.log(torch.tensor(x.size(0))) + 2*loss*x.size()[0]\n",
    "print(norm.cdf(-np.abs(effect/effect_std)))\n",
    "print(\"log lik: {:4.4f} \\n\".format(-loss.numpy()*x.size(0)))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69e914-b108-4862-97cb-9da681cc54b8",
   "metadata": {},
   "source": [
    "Now we save our results into a dataframe and write to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1a74f8c-5ecd-420d-98f8-f77e0c4d6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize the prediction results into a dataframe\n",
    "results = pd.DataFrame({\"gpr_mean\":mu_f/y_scale})\n",
    "results['true_y'] = y/y_scale\n",
    "results['gpr_lwr'] = lower/y_scale\n",
    "results['gpr_upr'] = upper/y_scale\n",
    "results['day'] = x[:,0].numpy().astype(int)\n",
    "results[\"cf_mean\"] = out0.mean.numpy() / y_scale\n",
    "\n",
    "results.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
