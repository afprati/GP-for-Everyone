{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64a29fa-ca4c-43fe-afba-2d6020e76274",
   "metadata": {},
   "source": [
    "This notebook describes the steps for the exile and online opposition example (second application). Data is from Esberg and Siegel 2023. Prepared by Annamaria Prati and Yehu Chen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3177a-b5b8-4563-a0d4-a68647774957",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Let's set ourselves up. Load the required libraries. Like before, note that `torch` should be version 2.6.0 and `gpytorch` should be version 1.8.1. Set the seed and the default data type to be `float64`. \n",
    "\n",
    "Given the size of the dataset, in this example we will implement a sparse GP model as an approximation for the full GP model (since GPs scale poorly). We will do this with inducing points and mini batches. Inducing points act as a compressed representation of the data, allowing for faster training and inference. More inducing points mean it will be closer to the full GP and therefore a better approximation, but it will be slower -- practitioners might want a lower number while they refine the model and increase the number of inducing points as they reach finalized production stages. Mini batches means that instead of using hte whole dataset at once, we divide it into smaller batches. Specifying the batch size controls how many training samples are processed at once during each optimization step. Using 256 is faster than using all the data at once, but is less noisy than very small batches might be. The number of empochs is the number of times the optimizer might go through the entire dataset during training (if the training loss stabilizes earlier, it might stop sooner). Note that one ``epoch'' is one full pass through the dataset via the mini batches. 100 is a standard starting point. \n",
    "\n",
    "In this case, we have set the number of inducing points to 3000, the batch size to 250, and the number of epochs to 100.\n",
    "\n",
    "Finally, load the dataset (we again provide it in a subfolder on the Github repo). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bc61f4-1cbe-4fd0-9382-9b49aa4fea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "# load gpytoch and other libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import ZeroMean, LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from datetime import datetime\n",
    "from gpytorch.means import Mean\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from typing import Optional, Tuple\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "print(gpytorch.__version__)\n",
    "\n",
    "# set a random seed so results are consistent each time you run the code\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# set default data type to float64 for accuracy\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# extra hyperparameters for the sparse GP model\n",
    "num_inducing = 3000\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "\n",
    "# load the dataset with the monthly tweet volumes\n",
    "# this file should be in a folder called 'data' with the name 'exile.csv'\n",
    "data = pd.read_csv(\"./data/exile.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd270a2-5f44-4435-828c-98c087f60b8a",
   "metadata": {},
   "source": [
    "We will also define for ourselves two helper functions for our month-year time series:\n",
    "\n",
    "- `diff_month`: computes the number of months between two dates.\n",
    "- `to_month`: converts a date into a monthly index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e5d214-1068-4cf9-b9cc-f07fc51fb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_month(d1, d2):\n",
    "    d1 = datetime.strptime(d1,\"%Y-%m-%d\")\n",
    "    d2 = datetime.strptime(d2,\"%Y-%m-%d\")\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "def to_month(d1):\n",
    "    return datetime(2013 + int(d1 / 12), ((1 +d1) % 12) + 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c461e69-3918-4404-994a-6392a0cbc58f",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "After only selecting the columns we needed, we construct our covariates: `month` is converted to a numerical index using our `diff_month` helper function to be the number of months since January 1 2013; `unit.id` is a categorical encoding based on `actor.id`; `log_num_tweets` is the logged number of tweets+1 for stability; and `tweeted_exile` is an indicator for whether the tweet was from exile. We also construct our tensor for the dependent variable.\n",
    "\n",
    "Since we are building a sparse GP, we will also define our inducing points at this stage. Because of the sparse GP, we also need to specify a training dataset (something we did not have to do in the first example on white nationalist rhetoric, where the covariance matrix was computed over all data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf0fbdb-7ec6-4b68-9bc3-e1bb67f48b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_name = \"perc_harsh_criticism\"\n",
    "\n",
    "data = data[[Y_name, \"tweeted_exile\", \"month\",\"num_tweets\", \"actor.id\"]] # only keeping columns we need\n",
    "\n",
    "# creating input data\n",
    "# xs: month, unit id, log_num_tweets, tweeted_exile\n",
    "xs = data.month.apply(lambda x: diff_month(x,\"2013-01-01\"))\n",
    "xs = torch.tensor(np.array([data[\"actor.id\"].astype('category').cat.codes.values.reshape((-1,)),\\\n",
    "            xs.values.reshape((-1,)),\n",
    "            np.log(data.num_tweets.values+1).reshape((-1,)), \\\n",
    "            data['tweeted_exile'].values.reshape((-1,))]).T)\n",
    "\n",
    "# creating the dependent variable\n",
    "Y_name = \"perc_harsh_criticism\" # percentage of tweets that are harsh criticism; facilitates changing dependent variable\n",
    "ys = torch.tensor(data[Y_name].values).double()\n",
    "\n",
    "# lets you convert numeric unit IDs back into original actor.id values later for interpretation\n",
    "to_unit = dict(enumerate(data[\"actor.id\"].astype('category').cat.categories))\n",
    "del data # to save memory\n",
    "\n",
    "# defining the training dataset\n",
    "train_dataset = TensorDataset(xs, ys)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a609b77-41a7-45a8-876e-26772f17a0dd",
   "metadata": {},
   "source": [
    "# Model Specification\n",
    "\n",
    "As explained in the main text, this model addresses three temporal modeling challenges:\n",
    "\n",
    "1. Shared temporal shocks affecting all activists; \n",
    "2. Different activists have different baseline propensities for criticism; and\n",
    "3. Activists have individual-specific trends in criticism that could confound the effect of exile\n",
    "\n",
    "\n",
    "\n",
    "We construct a GP model that addresses these challenges while allowing for flexible temporal dynamics. The model has three components. First, a basic GP prior captures the effect of exile and tweet volume:\n",
    "\\begin{eqnarray}\n",
    "    f \\sim \\mathcal{GP}\\big( \\mu_f(\\mathbf x_{it}), \\mathbf K_f \\big), \\text{ where}\\\\\n",
    "    \\mu_f(\\mathbf{x}_{it}) =  \\beta_0 + \\beta_1 \\text{exile}_{it} + \\beta_2 \\text{\\#tweets}_{it}. \n",
    "\\end{eqnarray}\n",
    "We use a squared exponential auto relevance determination (SE-ARD) kernel for $\\mathbf{K_f}$, allowing different weightings for the exile and tweet count predictors. Note that the SE-ARD kernel is a variation of the RBF kernel used earlier: Normally, the RBF kernel has a single lengthscale for all input dimensions. ARD is a special case where each dimension gets its own lengthscale. This makes ARD a kind of automatic feature selection built into the GP. This is usefule when we have multi-dimensional inputs, we don't know which inputs might matter most, and/or we want the GP to learn which dimensions are most relevant. \n",
    "\n",
    "Second, we model smooth common temporal shocks with:\n",
    "\\begin{eqnarray}\n",
    "    g(t) \\sim \\mathcal{GP}\\big(\\mathbf 0, \\textbf K_g \\big)\n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{K_g}$ is a SE kernel. This replaces any discrete monthly fixed effects with a continuous temporal process. Note that the SE kernel is widely used because it is relatively simple and encodes assumptions of smoothness and continuousness (it is infinitely differentiable), which is a reasonable starting point for many social sciences applications.\n",
    "\n",
    "Third, we capture unit-specific patterns through independent GP priors:\n",
    "\\begin{eqnarray}\n",
    "    h_i(t) \\sim \\mathcal{GP}\\big(\\mathbf \\mu_{h[i]}(t), \\textbf K_h\\big), \\text{ where}\\\\\n",
    "    \\mu_{h[i]}(t) = \\alpha_{1[i]} + \\alpha_{2[i]} t.\n",
    "\\end{eqnarray}\n",
    "This allows each unit to follow its own temporal trajectory while maintaining a shared degree of smoothness across units.\n",
    "\n",
    "We assume shared hyperparameters across units, which encodes the assumption that unit trends should exhibit similar levels of nonlinear deviations.\n",
    "\n",
    "The complete model combines these components with Gaussian error:\n",
    "\\begin{eqnarray}\n",
    "\\mathbf y \\sim \\mathcal{MVN}(\\boldsymbol \\mu_y, \\mathbf{K}_y), \\text{ where}\\\\\n",
    "\\boldsymbol \\mu_y = \\boldsymbol \\mu_f + \\boldsymbol \\mu_{h[i]} \\text{ and}\\\\\n",
    "\\mathbf{K}_y = \\mathbf{K}_f + \\mathbf{K}_g + \\mathbf{K}_h + \\mathbf{I}\\sigma^2_{\\text{noise}}.\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07582a9-0d7f-4012-a901-dd4f310eedb8",
   "metadata": {},
   "source": [
    "# Constructing the Model\n",
    "\n",
    "We construct in `gpytorch` the components of the model to match our specifications and our temporal modeling challenges. Again, this will be a sparse variational GP that has a shared linear mean, unit specific means, and a covariance structure tat allows for continuous features, group effects, and interactions.\n",
    "\n",
    "First, we define a `ConstantVectorMean` class, which allows for each unit to ave its own constant mean. It defines a custom mean function where the mean is not a single scalar but instead a vector of constants indexed by a categorical input. We initialize a learnable parameter called `constantvector` to be all zeros. It will take the categorical indices as the input, convert to integers, and use those as an index into the vector (in the `forward`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3877da-c19b-44c9-ab7d-e1ca5763dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None: \n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecfe35-cef7-46e0-9fed-c4c7fb1ae215",
   "metadata": {},
   "source": [
    "Next, we define the `MaskMean` class. It is a wrapper mean function that allows our mean to only depend on one input instead of all possible inputs, as defined by the `active_dims`. It will extract those dimensions and pass them to the base mean (in the `forward`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a20dcde-498d-4919-8e4e-f79dcb5fc817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4911a9-bf8a-44c6-978e-f9a4bb4da1cd",
   "metadata": {},
   "source": [
    "Now we can put it together in the `GPModel` class, which will be an approximate GP using variational inference.\n",
    "\n",
    "This variational GP set-up is defined by the `CholeskyVariationalDistribution` and `VariationalStrategy`. We have set the inducing point locations to be fixed (`learn_inducing_locations=False`). \n",
    "\n",
    "For the mean function, we have a shared linear mean using the first two dimensions of the data (month and unit ID) (`self.mean_module`). There is no intercept (`bias=False`), it is a pure weighted sum of our two features. We also create a list of LinearMeans, one per unit (`self.unit_mean`, like group fixed effects). \n",
    "\n",
    "There are two pieces to the covariance function. First is `self.covar_module`. For this piece, we are using a scaled SE kernel applied to the unit ID and the tweet volumes, where the kernel will learn a spearate lengthscale for each of the dimensions. `ScaleKernel` wraps the base kernel and learns an extra scalar variance parameter. This will model smooth similarity between observations based on the activist and their tweet volumes. Second is `self.t_covar_module`. It defines a product for whether the tweet is from exile. \n",
    "\n",
    "In the `forward`, we built the mean vector (the shared mean with the unit specific means) and the total covariance (sum of all kernels: continuous features, whether the tweet was in exile, and group-level correlation). It returns a `MultivariateNormal` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318f96d3-09a3-40eb-81d1-42aad9c9fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, unit_num):\n",
    "        # variational GP setup\n",
    "        self.unit_num = unit_num\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=False)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "\n",
    "        # linear mean\n",
    "        self.mean_module = LinearMean(input_size=(2), bias=False)\n",
    "        self.unit_mean = torch.nn.ModuleList([LinearMean(input_size=(1),bias=True) for _ in range(unit_num)])\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=(2), active_dims=[2,3]))\n",
    "        self.t_covar_module = ScaleKernel(RBFKernel(active_dims=[0])*RBFKernel(active_dims=[1]))\n",
    "        self.g_covar_module = ScaleKernel(RBFKernel(active_dims=[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x[:,2:]) \n",
    "        for i in range(self.unit_num):\n",
    "            mean_x[x[:,0]==i] += self.unit_mean[i](x[i,1].reshape((-1,1)))\n",
    "        covar_x =  self.covar_module(x) + self.t_covar_module(x)  + self.g_covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8deeb0-2c41-45d1-bd8d-4e9b83ff94b5",
   "metadata": {},
   "source": [
    "Since we are building a sparse GP, we will also define our inducing points at this stage. Because of the sparse GP, we also need to specify a training dataset (something we did not have to do in the first example on white nationalist rhetoric, where the covariance matrix was computed over all data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd31d03e-93f2-4ef1-961b-ab2ddd55bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the inducing points\n",
    "inducing_points = xs[np.random.choice(xs.size(0),num_inducing,replace=False),:]\n",
    "\n",
    "model = GPModel(inducing_points=inducing_points, unit_num=xs[:,0].unique().size()[0]).double()\n",
    "likelihood = GaussianLikelihood().double()\n",
    "del inducing_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0702a-8977-431e-9c8b-8b5ccb136093",
   "metadata": {},
   "source": [
    "We set the initial values for the model's hyperparameters in a dictionary and initialize, as well as initialize model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9983094e-467b-45e7-a24b-fa1fd57a2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial values for model parameters\n",
    "\n",
    "hypers = {\n",
    "'mean_module.weights': torch.tensor([0, 5]),\n",
    "'covar_module.outputscale': 9,\n",
    "'covar_module.base_kernel.lengthscale': torch.std(xs[:,2:4],axis=0),\n",
    "'t_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor([12]),\n",
    "'t_covar_module.outputscale': 4,\n",
    "'g_covar_module.base_kernel.lengthscale': torch.tensor([24]),\n",
    "'g_covar_module.outputscale': 9\n",
    "}    \n",
    "\n",
    "model = model.initialize(**hypers)\n",
    "\n",
    "\n",
    "# initialize model parameters\n",
    "model.t_covar_module.base_kernel.kernels[0].raw_lengthscale.requires_grad_(False)\n",
    "model.t_covar_module.base_kernel.kernels[0].lengthscale = 0.01\n",
    "\n",
    "likelihood.noise = 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae08bf0-b701-403d-b828-22aeb8181e3d",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Like before, we switch the model and likelihood into \"training\" mode. We again use the Adam optimizer, though now we have a learning rate of 0.1 We are optimizing all model parameters except the raw lengthscale of the first kernel inside `t_covar_module` (like before, it is frozen) as well as the likelihoood parameters. Since we are now modeling a sparse GP, we will use Variational inference using Evidence Lower Bound (ELBO) and mini batching. Finally, using `train_loader`, we run a loop to train our model. In each loop, we zero the gradients, run the GP model's forward pass (`output=model(x_batch)`), compute the negative ELBO loss, backpropagate the gradients, and then update the parameters with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e869c1-28a3-4328-b916-4969242ac4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miame\\anaconda3\\Lib\\site-packages\\gpytorch\\lazy\\triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2259.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 1 - Loss: 206.852\n",
      "Epoch 1 Iter 51 - Loss: 13.866\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': list(set(model.parameters()) \\\n",
    "                - {model.t_covar_module.base_kernel.kernels[0].raw_lengthscale,\\\n",
    "                })},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=ys.size(0))\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for j, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 50 == 0:\n",
    "            print('Epoch %d Iter %d - Loss: %.3f' % (i + 1, j+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06728295-9706-410c-988c-c22dc3caea7a",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "\n",
    "We freeze our learned hyperparameters by switching the model and likelihood into \"evaluation\" mode and obtain our posterior predictions for both sets of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18f2f33-684a-48c3-9795-efd647877ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and likelihood to evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = model(xs)\n",
    "    mll.combine_terms = True\n",
    "    loss = -mll(out, ys)\n",
    "    mu_f = out.mean.numpy()\n",
    "    lower, upper = out.confidence_region()\n",
    "\n",
    "\n",
    "# copy training tensor to test tensors and set exile to 1 and 0\n",
    "test_x1 = xs.clone().detach().requires_grad_(False)\n",
    "test_x1[:,3] = 1\n",
    "test_x0 = xs.clone().detach().requires_grad_(False)\n",
    "test_x0[:,3] = 0\n",
    "\n",
    "# in eval mode the forward() function returns posterioir\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = model(xs)\n",
    "    mll.combine_terms = False\n",
    "    loss, _ , _ = mll(out, ys)\n",
    "    loss = -loss*out.event_shape[0]\n",
    "    out1 = model(test_x1)\n",
    "    out0 = model(test_x0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58304b1-7681-4044-b399-a23158ed4b41",
   "metadata": {},
   "source": [
    "We can now compute our ATE and its uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40642d29-b834-4463-8dcb-eb9b4f148564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 6.852 +- 0.979\n",
      "\n",
      "model evidence: -181330.798 \n",
      "\n",
      "BIC: 362721.768 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute ATE and its uncertainty\n",
    "effect = out1.mean.numpy()[xs[:,3]==1].mean() - out0.mean.numpy()[xs[:,3]==1].mean()\n",
    "effect_std = np.sqrt((out1.variance.detach().numpy()[xs[:,3]==1].mean()\\\n",
    "                    +out0.variance.detach().numpy()[xs[:,3]==1].mean()))\n",
    "BIC = (3+2+1)*\\\n",
    "    torch.log(torch.tensor(xs.size()[0])) + 2*loss # *xs.size(0)/batch_size\n",
    "print(\"ATE: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e08bf7-37b4-41a7-9dec-6b98b2e7be74",
   "metadata": {},
   "source": [
    "# Saving the Results\n",
    "\n",
    "We can save our results to a dataframe and then export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6a037f-05cf-4db0-83a2-e95c33851f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results\n",
    "results = pd.DataFrame({\"gpr_mean\":mu_f})\n",
    "results['true_y'] = ys\n",
    "results['gpr_lwr'] = lower\n",
    "results['gpr_upr'] = upper\n",
    "results['month'] = np.array([to_month(x) for x in xs[:,1].numpy().astype(int)])\n",
    "results['unit'] = np.array([to_unit[x] for x in xs[:,0].numpy().astype(int)])\n",
    "results['exile'] = xs[:,3].numpy().astype(int)\n",
    "\n",
    "test_x0 = xs.clone().detach().requires_grad_(False)\n",
    "test_x0[:,3] = 0\n",
    "\n",
    "# in eval mode the forward() function returns posterioir\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out0 = model(test_x0)\n",
    "    lower, upper = out0.confidence_region()\n",
    "\n",
    "results['cf'] = out0.mean.numpy()\n",
    "results['cf_lower'] = lower\n",
    "results['cf_upper'] = upper\n",
    "\n",
    "if Y_name == \"perc_harsh_criticism\":\n",
    "    abbr = \"crit\"\n",
    "else:\n",
    "    abbr = \"repr\"\n",
    "results.to_csv(\"./results/exile_{}_fitted_gpr.csv\".format(abbr),index=False) #save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8291a8-eddf-4cd9-acc0-ae6b9a9d5bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
